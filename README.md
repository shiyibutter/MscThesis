# Msc thesis on gender bias in GPT-4
Research question: Does GPT-4 exhibit gender bias in the content it generates when tasked with creating fictional stories, and if so, what are the extent and nature of this bias?

## **20 sep 2023**
The goal is to investigate the presence and extent of gender bias in fictional stories generated by GPT-4. It would be interesting to compare zero-shot learning with few-shot learning. in case of few-shot learning, a diverse dataset of fictional stories and prompts to serve as a reference point are needed. This dataset includes a wide range of themes, genres, and gender-related content. Then GPT-4 can be fine-tuned and the impact of fine-tuning can be studied. Continously, metrics and criteria to identify gender bias in the generated stories need to be developed and defined (based on previous work and literature). These indicators include for example gender pronoun usage, character stereotypes, and unequal gender representation.

**Notes:**
- Currently, the task is to generate fictional stories or content. The generated text should possess a storytelling quality and incorporate characters, potentially specifying their genders. However, this could also be generating summaries/reviews/recommendations for instance.
- The perfomrmance of GPT-3 could be compared to GPT-4.
- For now, the focus is on gender bias as the initial point of exploration. Nonetheless, there are numerous other biases yet to be investigated.

**Related literature:**
- [Nguyen, D., Doğruöz, A. S., Rosé, C. P., & De Jong, F. (2016). Computational sociolinguistics: A survey. Computational linguistics, 42(3), 537-593.](https://direct.mit.edu/coli/article/42/3/537/1536/Computational-Sociolinguistics-A-Survey)
- [Lucy, L., & Bamman, D. (2021, June). Gender and representation bias in GPT-3 generated stories. In Proceedings of the Third Workshop on Narrative Understanding (pp. 48-55).](https://aclanthology.org/2021.nuse-1.5/)
- [Yu, Y., Zhuang, Y., Zhang, J., Meng, Y., Ratner, A., Krishna, R., ... & Zhang, C. (2023). Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. arXiv preprint arXiv:2306.15895.](https://arxiv.org/abs/2306.15895)
- [Dhingra, H., Jayashanker, P., Moghe, S., & Strubell, E. (2023). Queer people are people first: Deconstructing sexual identity stereotypes in large language models. arXiv preprint arXiv:2307.00101.](https://arxiv.org/abs/2307.00101)
- [Nielsen, E., Kirov, C., & Roark, B. (2023). Spelling convention sensitivity in neural language models. arXiv preprint arXiv:2303.03457.](https://arxiv.org/abs/2303.03457)
- [Sheng, E., Chang, K. W., Natarajan, P., & Peng, N. (2019). The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326.](https://arxiv.org/abs/1909.01326)
- [Sun, T., Webster, K., Shah, A., Wang, W. Y., & Johnson, M. (2021). They, them, theirs: Rewriting with gender-neutral english. arXiv preprint arXiv:2102.06788.](https://arxiv.org/abs/2102.06788)
- [Yang, K., Peng, N., Tian, Y., & Klein, D. (2022). Re3: Generating longer stories with recursive reprompting and revision. arXiv preprint arXiv:2210.06774.](https://arxiv.org/abs/2210.06774)
